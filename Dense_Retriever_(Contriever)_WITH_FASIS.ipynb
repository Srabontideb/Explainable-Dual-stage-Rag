{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZrFTmyec05Nizrz9VrFAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srabontideb/Explainable-Dual-stage-Rag/blob/main/Dense_Retriever_(Contriever)_WITH_FASIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DRIVE MOUNT**"
      ],
      "metadata": {
        "id": "ZiTpOyeF9AHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OHKrhBi47zH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup and Install Dependencies**"
      ],
      "metadata": {
        "id": "DeV5pUgI9ECp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers faiss-cpu"
      ],
      "metadata": {
        "id": "oEkqIJtY8i53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Contriever Model**"
      ],
      "metadata": {
        "id": "Jh1Hw9nr9NwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow==10.2.0"
      ],
      "metadata": {
        "id": "9ldBhG7a8k7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/contriever\")\n",
        "model = AutoModel.from_pretrained(\"facebook/contriever\")\n",
        "model.eval()\n",
        "\n",
        "def embed_text(texts):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).last_hidden_state[:, 0, :]\n",
        "    return embeddings.cpu().numpy()"
      ],
      "metadata": {
        "id": "vvlULoQd8qVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load libraries**"
      ],
      "metadata": {
        "id": "r7ykkwV29c-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ‚úÖ Load dataset\n",
        "csv_path = \"/content/drive/MyDrive/Colab Notebooks/cleaned_and_tokenized_and_entity-defined_healthcaremagic.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "corpus = df[\"response_clean\"].dropna().tolist()\n",
        "\n",
        "# ‚úÖ Load Contriever model + tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/contriever\")\n",
        "model = AutoModel.from_pretrained(\"facebook/contriever\")\n",
        "model.eval()  # CPU only"
      ],
      "metadata": {
        "id": "ihHrblV18rr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embed Corpus + Create FAISS Index**"
      ],
      "metadata": {
        "id": "tromnAW99hQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ‚úÖ Ensure save directory exists\n",
        "save_dir = \"/content/drive/MyDrive/faiss_chunks\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ‚úÖ Define embedding function if not defined yet\n",
        "def embed_texts(texts, batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding chunk\"):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            output = model(**inputs).last_hidden_state[:, 0, :]\n",
        "        embeddings.append(output.cpu().numpy())\n",
        "    return np.vstack(embeddings)\n"
      ],
      "metadata": {
        "id": "t7LFhBZr8xt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUN MODEL FOR FIRST 10K**"
      ],
      "metadata": {
        "id": "IG2HUNrB9lar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Parameters\n",
        "chunk_size = 10000\n",
        "embedding_dim = 768  # For Contriever\n",
        "\n",
        "for i in range(0, len(corpus), chunk_size):\n",
        "    chunk_texts = corpus[i:i + chunk_size]\n",
        "    print(f\"üîπ Processing chunk {i} to {i+len(chunk_texts)}\")\n",
        "\n",
        "    # Embed chunk\n",
        "    chunk_embeddings = embed_texts(chunk_texts)\n",
        "\n",
        "    # Create new FAISS index for this chunk\n",
        "    chunk_index = faiss.IndexFlatL2(embedding_dim)\n",
        "    chunk_index.add(chunk_embeddings)\n",
        "\n",
        "    # Save index chunk\n",
        "    chunk_path = f\"{save_dir}/faiss_chunk_{i}_{i + len(chunk_texts)}.faiss\"\n",
        "    faiss.write_index(chunk_index, chunk_path)\n",
        "    print(f\"‚úÖ Saved chunk index ‚Üí {chunk_path}\")\n"
      ],
      "metadata": {
        "id": "GhJkbRLX8yb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUN MODEL FOR ANY NEXT 10K(JUST CHANGE THE STATING INDEX)**"
      ],
      "metadata": {
        "id": "GU7dpd7r9rY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Parameters\n",
        "chunk_size = 10000\n",
        "embedding_dim = 768  # For Contriever\n",
        "save_dir = \"/content/drive/MyDrive/faiss_chunks\"  # Your save location\n",
        "\n",
        "# ‚úÖ Manually set where to start\n",
        "start_idx = 10000   # Change to your last completed + 1\n",
        "end_idx = len(corpus)  # or set manually if testing\n",
        "\n",
        "for i in range(start_idx, end_idx, chunk_size):\n",
        "    chunk_texts = corpus[i:i + chunk_size]\n",
        "    print(f\"üîπ Processing chunk {i} to {i+len(chunk_texts)}\")\n",
        "\n",
        "    # Embed chunk\n",
        "    chunk_embeddings = embed_texts(chunk_texts)\n",
        "\n",
        "    # Create new FAISS index for this chunk\n",
        "    chunk_index = faiss.IndexFlatL2(embedding_dim)\n",
        "    chunk_index.add(chunk_embeddings)\n",
        "\n",
        "    # Save index chunk\n",
        "    chunk_path = f\"{save_dir}/faiss_chunk_{i}_{i + len(chunk_texts)}.faiss\"\n",
        "    faiss.write_index(chunk_index, chunk_path)\n",
        "    print(f\"‚úÖ Saved chunk index ‚Üí {chunk_path}\")\n"
      ],
      "metadata": {
        "id": "4P5auL5p81CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEXT EXTRA**"
      ],
      "metadata": {
        "id": "5GKmCVnj94M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Parameters\n",
        "chunk_size = 10000\n",
        "embedding_dim = 768  # For Contriever\n",
        "batch_size = 256     # Smaller = more frequent saves to memory\n",
        "save_dir = \"/content/drive/MyDrive/faiss_chunks\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ‚úÖ Where to resume\n",
        "start_idx = 10000  # Change to last saved + 1\n",
        "end_idx = len(corpus)\n",
        "\n",
        "for i in range(start_idx, end_idx, chunk_size):\n",
        "    chunk_texts = corpus[i:i + chunk_size]\n",
        "    chunk_filename = f\"faiss_chunk_{i}_{i + len(chunk_texts)}.faiss\"\n",
        "    chunk_path = os.path.join(save_dir, chunk_filename)\n",
        "\n",
        "    if os.path.exists(chunk_path):\n",
        "        print(f\"‚è© Skipping existing chunk ‚Üí {chunk_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"üîπ Processing chunk {i} to {i + len(chunk_texts)}\")\n",
        "\n",
        "    try:\n",
        "        # Create empty FAISS index\n",
        "        chunk_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "        # Embed in mini-batches\n",
        "        for j in tqdm(range(0, len(chunk_texts), batch_size), desc=\"Embedding mini-batches\"):\n",
        "            batch_texts = chunk_texts[j:j + batch_size]\n",
        "            batch_embeddings = embed_texts(batch_texts)  # returns np.array\n",
        "            chunk_index.add(batch_embeddings)\n",
        "\n",
        "        # Save this chunk\n",
        "        faiss.write_index(chunk_index, chunk_path)\n",
        "        print(f\"‚úÖ Saved chunk index ‚Üí {chunk_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing chunk {i} ‚Üí {e}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "-3jXfR058-FP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}