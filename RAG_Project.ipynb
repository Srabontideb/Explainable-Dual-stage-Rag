{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHSUO1UKZPn0",
    "outputId": "c551fcc6-3eae-4420-97d8-5ee0c18f649a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7kVWFKabVSk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66eJg_0dbOrJ"
   },
   "outputs": [],
   "source": [
    "input_file = '/content/drive/MyDrive/HealthCareMagic-100k-en.jsonl'\n",
    "output_dir = '/content/drive/MyDrive/healthcare_rag_chunks'\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH0Zx-FAbaSK"
   },
   "source": [
    "## **Load Conversations from JSONL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ET7NIWWXbOiJ",
    "outputId": "72d31377-5256-45bd-cb5b-2c86b75d49d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 conversations...\n",
      "Loaded 2000 conversations...\n",
      "Loaded 3000 conversations...\n",
      "Loaded 4000 conversations...\n",
      "Loaded 5000 conversations...\n",
      "Total loaded: 5000\n"
     ]
    }
   ],
   "source": [
    "def load_conversations(file_path, limit=5000):\n",
    "    \"\"\"Read conversations from JSONL file\"\"\"\n",
    "    conversations = []        # Initialize an empty list to store the conversations\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:     # Open the file in read mode with UTF-8 encoding\n",
    "        for idx, line in enumerate(f):   # Iterate over each line in the file\n",
    "            if idx >= limit:  # Stop after reaching the limit (default is 5000)\n",
    "                break\n",
    "\n",
    "            data = json.loads(line.strip()) # Parse the JSON data from the line, removing any leading/trailing whitespace\n",
    "\n",
    "            # Extract conversation text from JSON\n",
    "            if isinstance(data, str):  # If the data is a string (conversation text)\n",
    "                conversations.append(data)  # Add it to the list\n",
    "            elif isinstance(data, dict):   # If the data is a dictionary (usually JSON structure)\n",
    "               # Try common field names to locate the conversation content\n",
    "                for field in ['text', 'conversation', 'content']:\n",
    "                    if field in data:     # Check if the field exists in the dictionary\n",
    "                        conversations.append(data[field])  # Add the conversation content to the list\n",
    "                        break # Stop once we find the relevant field\n",
    "\n",
    "            # Every 1000 conversations, print a status update\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"Loaded {idx + 1} conversations...\")\n",
    "\n",
    "    print(f\"Total loaded: {len(conversations)}\") # Print the total number of loaded conversations\n",
    "    return conversations # Return the list of conversations\n",
    "\n",
    "# Load data\n",
    "conversations = load_conversations(input_file, limit=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rji_vF47bsbB"
   },
   "source": [
    "## **Chunk a Single Conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH7EvU4XboCS"
   },
   "outputs": [],
   "source": [
    "def chunk_conversation(text, conv_id):\n",
    "    \"\"\"Split one conversation into chunks (question + answer pairs)\"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    # Split by <human>: to separate each question\n",
    "    parts = re.split(r'(?=<human>:)', text)\n",
    "\n",
    "    for turn_idx, part in enumerate(parts):\n",
    "        part = part.strip()\n",
    "        if not part or '<human>:' not in part: #Skips empty pieces and any segment that doesn’t actually contain <human>: (guards against leading text or artifacts).\n",
    "            continue\n",
    "\n",
    "        # Extract question (between <human>: and <bot>:)\n",
    "        question_match = re.search(r'<human>:\\s*(.*?)(?=<bot>:|$)', part, re.DOTALL) #re.DOTALL makes . match newlines, so multi-line questions are allowed.\n",
    "        if not question_match:\n",
    "            continue\n",
    "\n",
    "        question = question_match.group(1).strip()\n",
    "\n",
    "        # Extract answer (between <bot>: and next <human>: or end)\n",
    "        answer_match = re.search(r'<bot>:\\s*(.*?)(?=<human>:|$)', part, re.DOTALL)\n",
    "        answer = answer_match.group(1).strip() if answer_match else \"\"\n",
    "\n",
    "        # Create chunk\n",
    "        chunk = {\n",
    "            'chunk_id': f\"conv_{conv_id}_turn_{turn_idx}\",\n",
    "            'conversation_id': conv_id,\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'full_text': part.strip()\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxOlQobrb0NS",
    "outputId": "05541ec1-4a2e-4c46-8295-5af5f49161da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing conversations...\n",
      "Processed 500 conversations → 500 chunks\n",
      "Processed 1000 conversations → 1000 chunks\n",
      "Processed 1500 conversations → 1500 chunks\n",
      "Processed 2000 conversations → 2000 chunks\n",
      "Processed 2500 conversations → 2500 chunks\n",
      "Processed 3000 conversations → 3000 chunks\n",
      "Processed 3500 conversations → 3500 chunks\n",
      "Processed 4000 conversations → 4000 chunks\n",
      "Processed 4500 conversations → 4500 chunks\n",
      "Processed 5000 conversations → 5000 chunks\n",
      "\n",
      "✅ Total chunks created: 5000\n"
     ]
    }
   ],
   "source": [
    "# Process all conversations\n",
    "all_chunks = []\n",
    "\n",
    "print(\"Processing conversations...\")\n",
    "for idx, conv in enumerate(conversations):\n",
    "    chunks = chunk_conversation(conv, conv_id=idx)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"Processed {idx + 1} conversations → {len(all_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n✅ Total chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbpxar2Hb3hl",
    "outputId": "8f08e8ce-e553-4103-ee42-e65da555453d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PREVIEW ---\n",
      "\n",
      "Chunk 1:\n",
      "Question: I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!\n",
      "Answer: Hi, Thank you for posting your query. The most likely cause for your symptoms is benign paroxysmal positional vertigo (BPPV), a type of peripheral vertigo. In this condition, the most common symptom is dizziness or giddiness, which is made worse with movements. Accompanying nausea and vomiting are common. The condition is due to problem in the ear, and improves in a few days on own. Betahistine tablets would help relieve your symptoms. Doing vestibular rehabilitation or adaptation exercises would prevent the recurrence of these symptoms. An ENT evaluation would also help. I hope it helps. Best wishes, Chat Doctor.\n",
      "\n",
      "Chunk 2:\n",
      "Question: My baby has been pooing 5-6 times a day for a week. In the last few days it has increased to 7 and they are very watery with green stringy bits in them. He does not seem unwell i.e no temperature and still eating. He now has a very bad nappy rash from the pooing ...help!\n",
      "Answer: Hi... Thank you for consulting in Chat Doctor. It seems your kid is having viral diarrhea. Once it starts it will take 5-7 days to completely get better. Unless the kids having low urine output or very dull or excessively sleepy or blood in motion or green bilious vomiting...you need not worry. There is no need to use antibiotics unless there is blood in the motion. Antibiotics might worsen if unnecessarily used causing antibiotic associated diarrhea. I suggest you use zinc supplements (Z&D Chat Doctor.\n"
     ]
    }
   ],
   "source": [
    "# Show first 2 chunks\n",
    "print(\"\\n--- PREVIEW ---\")\n",
    "for i in range(min(2, len(all_chunks))):\n",
    "    chunk = all_chunks[i]\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Question: {chunk['question']}\")\n",
    "    print(f\"Answer: {chunk['answer'] if chunk['answer'] else 'No answer'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpfqYdV1a0_9",
    "outputId": "1d30e41c-4d3f-4ebc-df9d-89b45958782f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25, faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0 rank-bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers faiss-cpu rank-bm25 transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPufiE5ZsfJH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "import pickle\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5-YQxMBy9oi"
   },
   "source": [
    "## **Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585,
     "referenced_widgets": [
      "883f2f83eef645fba0b78660428a9569",
      "c6ad2bfad14c44ebb94831867b0dd695",
      "8ea8426ea5804e4e89d27a8a69833951",
      "4598b775eee943a2b2dce588db26abe1",
      "7d493b148ae64e10b26ffe1325301653",
      "b69df598f4cd4ed3b47679284f2568f2",
      "65280408d75340aba3a4a2a95bccb91d",
      "d9a03299f8d24d8a8428f4a39af82d42",
      "593cbafd7beb47e4b76869ac5822e2c1",
      "e1a84066a01543b9bde67149f043aa42",
      "fc5762ea66f64ac78491432b3fd3c164",
      "abb7e7c4da2e4f50ba50158f73583d83",
      "9e92e1862ac44a34bf1d76281b66d646",
      "7bf878bc9b8d47e894cd2aad5146a058",
      "e1bb93d6e376488590ceabb1632b7716",
      "13998de802844fd79dd1e95f35e29d04",
      "06f5edc942034d61afad6ebc79f83ae2",
      "8156c19e8ba145c899c959da0ea94361",
      "9002697dda50433b934ffc975bc8b345",
      "7c0e850f2ebd4dd2abf8dafe6e767a58",
      "c0272136e8644bbfa22afc225860f2a0",
      "fdce3cd3a37c43dba69db96b1397d8b0",
      "38f39c0f9bd6432fa4d64abb0baa4b45",
      "be83698fc24e4f61a2e2b12c0a654031",
      "bf49fdc8c13d466fa813a40484bbe71a",
      "50f1bf1c31504532afd4e7228f6c393d",
      "63170f90e73049bdac014fab01390210",
      "89d333685ecd40a0b707bcf1a069ffc3",
      "b312afe4104b45bbb73aaabca62847a2",
      "edeb0945b9d14901a3260b20fcbae590",
      "8f7a4834bbe343b59d789b77473d2148",
      "506c2bf9a7464b5ca407549663e9956a",
      "f4068d8a84e94407a5cf63e108d5f82d",
      "58fcd8a9478d4f7cb4bae589ed4c08d2",
      "81791ff223c54fa4896c1ed0e15d79e1",
      "7885ec93973448aa8bbd5cf73c6b7564",
      "93168d13b9cb49d7a48382d767ae0122",
      "5475104b01bb45978be16b7b75c58204",
      "0729e3dab5204ce38d391bc96c1cccef",
      "4da7ef8c13e249feb7b4f6d149fb6ebc",
      "1ba910ea3ac9401c9d76bda9b5bd9cbc",
      "665e4550e9e746b9b1cec70cb5f1b377",
      "459730ae19ec4608bdb990681ffd5f44",
      "c5dfca70a38b4e03b88c09dfd6023d52",
      "9e21659dcb7f4f56885b85e24f43c628",
      "ef2a6964daa94a6d93634677314cda7d",
      "da575fef26784ea3881244f4583be1e6",
      "2aba7cec4f7d4c0280061a5ffbd39bcb",
      "ce3c4f9ba0a94a0bb0e3ade13cdc62d7",
      "ac010d1be9694067bafa851e2f52dbe4",
      "6c705a3a59a44e6e83d51ae1a77e4cc5",
      "c7793eff2a5244868700a82be3555477",
      "75c711c4ab084bf4aba7a0ea8feed862",
      "2a38d3b9dffb4c86a8303be88c8554a3",
      "c4bc8b24037c4633ade26f3aea23b0cf",
      "917492cd4c21438087fe4404843ef542",
      "5164565465154de999252963cef94c2d",
      "bf3857f1ae4244338ee7b143573db02a",
      "67456b08e7684a13ada6267ba7a27a72",
      "10ea812aebd244668a59f0b572e4415e",
      "3ceab4cb88754cac8516592f39a5b275",
      "87d083c55df04022ae37f44ca7af0791",
      "3eb5c6a056d848f3ae28a1057dbf60ef",
      "fb24c218d45c415e8533d1a4f68dfa9a",
      "ff2078c9eadf4703bd167787c26ae272",
      "5f7f5bb69f6b4200ba164c44fcf8e8d6",
      "c3ec4ad33ef44b118aa7e297c00f5fee",
      "b1fd163cb1d94ccbb683361a4b9b44dc",
      "54763ef1ec5240389f23ff721db19825",
      "3ac2a277471141c18f9f5a85faa1b914",
      "6b59c85ea3f5479399dc78eca91db99a",
      "40f85d7fa1d94bfbad6e7d8a9fe98746",
      "4316ad95a4e74b86bb304228e5e4d594",
      "527583ac9ec248109f94c6eae09f15b2",
      "9e6c9f57b0b540e8a9ead6761f007fb5",
      "862b5dceb88a4926b19af5f5a129a618",
      "07a6800cde6a4bc4847e6f7900116720",
      "333033b8997e4846993536e65e19b883",
      "e0241f3e7fc84fb0ba91bdd121ba258f",
      "80685bb12bca4963aa0728813e390caf",
      "2872d17a08014a3aa807f398519714af",
      "0d4460253a724a66966f3c0b14ec9fe3",
      "25f449f5a1604663ad955daabf241c1d",
      "9b035f1759604c279582f417ea2e2328",
      "feeda8c24dbb494ca630cd6dea287c73",
      "22778bcd08ef461ea60bb4021a1fa580",
      "87af17d849e94411b8fb54b040a38ae4",
      "33d126a4765f402ca2a07b19af81c067",
      "9d13ccf8bd5e4727acae9bb240fac656",
      "4786d42e923440fb8b57e03f140d979e",
      "eb555e18e1a1401d8a1224abafea9d13",
      "5232ff794bd74d1e85daba0634607044",
      "599971fc972742539002e343b2b82569",
      "52ce675909344886a4fb7c3736e86b78",
      "3cedeec883674d4f9bb8fc8d7a2d2d77",
      "33238456efaf4d2ea37ed95cdddac47f",
      "fa41b6f95c2044069aebb86623fe9e2b",
      "bf13559685dc45c1a1b4d11a87ee7703",
      "d0b8f7c435924f3d97d7b71e68cf8699",
      "b35cd70dc360458880624dff2a8908df",
      "a35d024092a74be4a6063c15a6c297ca",
      "52f291e3858d4eea8d3cef8ed5f26583",
      "d323b6e2018a43dc885997b727ad1d36",
      "20255c8c57a044c9a36052c726c069c3",
      "d43c184dd0b54f729034082f12fbbacd",
      "e3708c6476224b3a9cb86a60ec744e66",
      "2e7415a776a4405e9e0b3b73a62dadaa",
      "b892f2a6ed2b4353849dd7063b249aed",
      "dd3fa44ea3d64ff0b7c812441cc7ba7a",
      "16921fef594643b48685ff4e26eb4cf5",
      "69c8171eeb484239bd8571e2a6cbfbf5",
      "fa8ebbea128c449086f2791d60cd0185",
      "2510ac2b8d474a2c9999a13ee50b9902",
      "2852cdd9b6f046c1aff2d70c427aa5e6",
      "1b1043601e5341a6a8ae852fdcf6fc87",
      "669304d32f7b400b86702863f2e03e48",
      "8aaec7a3c0a646ea8d8e1d3653e46583",
      "c4758aa700a543a0aa00adea6fbf96e8",
      "6fc283cf2e4d432b9c969cc995bd85d3",
      "813b5457b3314911a64aac6fc0659c74",
      "eb1302a14dc54f8aba81122093ae67a1"
     ]
    },
    "id": "s811Giays91H",
    "outputId": "29a6d5a1-73ec-4c2d-e9f0-8c45ffcdf13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883f2f83eef645fba0b78660428a9569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb7e7c4da2e4f50ba50158f73583d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f39c0f9bd6432fa4d64abb0baa4b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fcd8a9478d4f7cb4bae589ed4c08d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e21659dcb7f4f56885b85e24f43c628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917492cd4c21438087fe4404843ef542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ec4ad33ef44b118aa7e297c00f5fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333033b8997e4846993536e65e19b883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d13ccf8bd5e4727acae9bb240fac656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35cd70dc360458880624dff2a8908df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c8171eeb484239bd8571e2a6cbfbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded! Embedding dimension: 384\n",
      "\n",
      "Prepared 5000 texts for embedding\n"
     ]
    }
   ],
   "source": [
    "# Choose embedding model\n",
    "EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "print(f\"\\nLoading embedding model: {EMBEDDING_MODEL}\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "print(f\"✅ Model loaded! Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "\n",
    "#Prepare texts for embedding (combining question + answer)\n",
    "def prepare_texts_for_embedding(chunks):\n",
    "    \"\"\"Prepare text from chunks for embedding\"\"\"\n",
    "    texts = []\n",
    "    for chunk in chunks:\n",
    "        combined_text = f\"Question: {chunk['question']}\\nAnswer: {chunk['answer']}\"\n",
    "        texts.append(combined_text)\n",
    "    return texts\n",
    "\n",
    "texts_to_embed = prepare_texts_for_embedding(all_chunks)\n",
    "print(f\"\\nPrepared {len(texts_to_embed)} texts for embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "146a21a59bca431fa951746ee7a7b0ed",
      "7ce869bbab9548c49b77afff74ed07fe",
      "07560279ec2d4e7a883e1f8d9e25f32f",
      "7eb45a70213043539de2aec31f2af8b9",
      "195ea6abf466425dbafe33aaa3812ad8",
      "df637c257742457abd78d9d45553466e",
      "eccb8989feb648d0805a4d7295b06c10",
      "6b01f2409a3e421887a9999904fef821",
      "dfd0b446de61494f8b946ce56a2928ee",
      "d68ee72527504ad78fa21f90fbf1e7e0",
      "052f89bc5c584bd8ba9134e3006d8e7c"
     ]
    },
    "id": "cYnEzfTVuMJ1",
    "outputId": "bf912af9-fe39-4532-bf9f-550dc9a19fe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146a21a59bca431fa951746ee7a7b0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings generated!\n",
      "Shape: (5000, 384)\n",
      "Memory size: 7.32 MB\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "print(\"\\nGenerating embeddings...\")\n",
    "embeddings = embedding_model.encode(\n",
    "    texts_to_embed,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # Normalize for cosine similarity\n",
    ")\n",
    "\n",
    "print(f\"✅ Embeddings generated!\")\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Memory size: {embeddings.nbytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNnxNU8YuTA1",
    "outputId": "40bbb7df-7397-4ca2-857b-e4fac3cdad58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created with 5000 vectors\n",
      "✅ FAISS index saved to: /content/drive/MyDrive/healthcare_rag_chunks/faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# STORE EMBEDDINGS IN FAISS AND CREATE INDEX\n",
    "# ========================================================\n",
    "\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Using IndexFlatIP for Inner Product (cosine similarity with normalized vectors)\n",
    "faiss_index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# Add embeddings to index\n",
    "faiss_index.add(embeddings.astype('float32'))\n",
    "print(f\"✅ FAISS index created with {faiss_index.ntotal} vectors\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss_index_path = os.path.join(output_dir, 'faiss_index.bin')\n",
    "faiss.write_index(faiss_index, faiss_index_path)\n",
    "print(f\"✅ FAISS index saved to: {faiss_index_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szFL3Q1op7ES",
    "outputId": "f3f1586f-93d4-4528-eda7-2dae5d720c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 5000 documents\n",
      "✅ BM25 index created\n",
      "✅ BM25 index saved to: /content/drive/MyDrive/healthcare_rag_chunks/bm25_index.pkl\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# PREPARE BM25 FOR KEYWORD SEARCH\n",
    "# ========================================================\n",
    "\n",
    "\n",
    "# Tokenize texts for BM25\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple tokenization for BM25\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "tokenized_corpus = [simple_tokenize(text) for text in texts_to_embed]\n",
    "print(f\"Tokenized {len(tokenized_corpus)} documents\")\n",
    "\n",
    "# Create BM25 index\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(\"✅ BM25 index created\")\n",
    "\n",
    "# Save BM25 index\n",
    "bm25_path = os.path.join(output_dir, 'bm25_index.pkl')\n",
    "with open(bm25_path, 'wb') as f:\n",
    "    pickle.dump(bm25, f)\n",
    "print(f\"✅ BM25 index saved to: {bm25_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An8r21Q3Rrf_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-u4UEf4a1fg"
   },
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "#  LOAD RERANKER MODEL\n",
    "# ========================================================\n",
    "\n",
    "# Load cross-encoder for reranking\n",
    "RERANKER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "print(f\"Loading reranker: {RERANKER_MODEL}\")\n",
    "reranker = CrossEncoder(RERANKER_MODEL)\n",
    "print(\"✅ Reranker loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWZlMK-P6537"
   },
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# HYBRID SEARCH FUNCTION WITH RRF (Reciprocal Rank Fusion)\n",
    "# ========================================================\n",
    "\n",
    "def semantic_search(query, top_k=20):\n",
    "    \"\"\"Perform semantic search using FAISS\"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [query],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # Search in FAISS\n",
    "    scores, indices = faiss_index.search(query_embedding.astype('float32'), top_k)\n",
    "\n",
    "    results = []\n",
    "    for rank, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "        results.append({\n",
    "            'chunk_idx': int(idx),\n",
    "            'score': float(score),\n",
    "            'rank': rank + 1,  # Rank starts from 1\n",
    "            'source': 'semantic'\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def keyword_search(query, top_k=20):\n",
    "    \"\"\"Perform keyword search using BM25\"\"\"\n",
    "    tokenized_query = simple_tokenize(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(top_indices):\n",
    "        results.append({\n",
    "            'chunk_idx': int(idx),\n",
    "            'score': float(scores[idx]),\n",
    "            'rank': rank + 1,  # Rank starts from 1\n",
    "            'source': 'keyword'\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(semantic_results, keyword_results, k=60):\n",
    "    \"\"\"\n",
    "    Combine search results using Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "    RRF Formula: score(d) = sum(1 / (k + rank(d)))\n",
    "    where k is a constant (typically 60) and rank(d) is the rank of document d\n",
    "\n",
    "    Args:\n",
    "        semantic_results: List of results from semantic search\n",
    "        keyword_results: List of results from keyword search\n",
    "        k: RRF constant (default: 60)\n",
    "\n",
    "    Returns:\n",
    "        List of fused results sorted by RRF score\n",
    "    \"\"\"\n",
    "    # Create rank dictionaries\n",
    "    semantic_ranks = {result['chunk_idx']: result['rank'] for result in semantic_results}\n",
    "    keyword_ranks = {result['chunk_idx']: result['rank'] for result in keyword_results}\n",
    "\n",
    "    # Create a set of all unique document IDs\n",
    "    all_doc_ids = set(semantic_ranks.keys()) | set(keyword_ranks.keys())\n",
    "\n",
    "    # Calculate RRF scores for all documents\n",
    "    reranked_scores = {}\n",
    "    for doc_id in all_doc_ids:\n",
    "        # Get the ranks, defaulting to infinity if the document wasn't in a list\n",
    "        semantic_rank = semantic_ranks.get(doc_id, float('inf'))\n",
    "        keyword_rank = keyword_ranks.get(doc_id, float('inf'))\n",
    "\n",
    "        # Calculate the RRF score\n",
    "        rrf_score = (1 / (k + semantic_rank)) + (1 / (k + keyword_rank))\n",
    "        reranked_scores[doc_id] = rrf_score\n",
    "\n",
    "    # Sort the documents by their RRF score in descending order\n",
    "    fused_results = []\n",
    "    for doc_id, rrf_score in sorted(reranked_scores.items(), key=lambda item: item[1], reverse=True):\n",
    "        fused_results.append({\n",
    "            'chunk_idx': doc_id,\n",
    "            'rrf_score': rrf_score,\n",
    "            'semantic_rank': semantic_ranks.get(doc_id, None),\n",
    "            'keyword_rank': keyword_ranks.get(doc_id, None)\n",
    "        })\n",
    "\n",
    "    return fused_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqPwFAnD7CnC"
   },
   "outputs": [],
   "source": [
    "def hybrid_search(query, top_k_semantic=20, top_k_keyword=20, rrf_k=60, final_top_k=10):\n",
    "    \"\"\"\n",
    "    Perform hybrid search combining semantic and keyword search using RRF + Cross-Encoder Reranking\n",
    "\n",
    "    Pipeline:\n",
    "    1. Semantic Search (FAISS) → top_k_semantic results\n",
    "    2. Keyword Search (BM25) → top_k_keyword results\n",
    "    3. Reciprocal Rank Fusion (RRF) → combine and rank\n",
    "    4. Cross-Encoder Reranking → final ranking\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: '{query[:100]}...'\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Step 1: Semantic search\n",
    "    # print(\"1️⃣  Performing semantic search...\")\n",
    "    semantic_results = semantic_search(query, top_k=top_k_semantic)\n",
    "    # print(f\"   Retrieved {len(semantic_results)} results from FAISS\")\n",
    "\n",
    "    # Step 2: Keyword search (BM25)\n",
    "    # print(\"2️⃣  Performing keyword search (BM25)...\")\n",
    "    keyword_results = keyword_search(query, top_k=top_k_keyword)\n",
    "    # print(f\"   Retrieved {len(keyword_results)} results from BM25\")\n",
    "\n",
    "    # Step 3: Reciprocal Rank Fusion (RRF)\n",
    "    # print(f\"3️⃣  Applying Reciprocal Rank Fusion (k={rrf_k})...\")\n",
    "    fused_results = reciprocal_rank_fusion(semantic_results, keyword_results, k=rrf_k)\n",
    "    # print(f\"   Combined {len(fused_results)} unique chunks\")\n",
    "\n",
    "    # Step 4: Cross-Encoder Reranking\n",
    "    # print(\"4️⃣  Reranking with Cross-Encoder...\")\n",
    "\n",
    "    # Prepare pairs for reranking (use top candidates from RRF)\n",
    "    # We'll rerank more than final_top_k to get better results\n",
    "    candidates_for_reranking = min(len(fused_results), final_top_k * 3)  # Rerank 3x the final amount\n",
    "\n",
    "    rerank_pairs = []\n",
    "    chunk_indices = []\n",
    "\n",
    "    for result in fused_results[:candidates_for_reranking]:\n",
    "        idx = result['chunk_idx']\n",
    "        chunk = all_chunks[idx]\n",
    "        chunk_text = f\"Question: {chunk['question']}\\nAnswer: {chunk['answer']}\"\n",
    "        rerank_pairs.append([query, chunk_text])\n",
    "        chunk_indices.append(idx)\n",
    "\n",
    "    # Get cross-encoder reranking scores\n",
    "    rerank_scores = reranker.predict(rerank_pairs)\n",
    "\n",
    "    # Combine RRF scores and reranker scores\n",
    "    final_results = []\n",
    "    for idx, rerank_score, rrf_result in zip(chunk_indices, rerank_scores, fused_results[:candidates_for_reranking]):\n",
    "        final_results.append({\n",
    "            'chunk_idx': idx,\n",
    "            'rrf_score': rrf_result['rrf_score'],\n",
    "            'rerank_score': float(rerank_score),\n",
    "            'semantic_rank': rrf_result['semantic_rank'],\n",
    "            'keyword_rank': rrf_result['keyword_rank'],\n",
    "            'chunk': all_chunks[idx]\n",
    "        })\n",
    "\n",
    "    # Sort by cross-encoder reranking score (final ranking)\n",
    "    final_results.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "\n",
    "    # Return top-k after reranking\n",
    "    top_results = final_results[:final_top_k]\n",
    "\n",
    "    # print(f\"✅ Retrieved top {len(top_results)} chunks after reranking\")\n",
    "    # print(f\"   Top result - RRF: {top_results[0]['rrf_score']:.4f}, Rerank: {top_results[0]['rerank_score']:.4f}\")\n",
    "\n",
    "    return top_results\n",
    "\n",
    "\n",
    "# print(\"✅ Hybrid search function with RRF defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZKb8nxwcdNy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DF-f0xU7EbL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "4cb878bb3455435387255bfba0bdadcd",
      "6c1d81490f2e4cd2882f6893a94c57b4",
      "ac589d6f7bcb487cb882279468f5f665",
      "3af21ec825664443aa784285491fabc9",
      "77f61a7de0424f47a058db5e29a3cc05",
      "d5f57fdcbc644e0faa6556585215bb7f",
      "20a8b9e0e57246f1ae076768295b45a6",
      "a7a018c2f99c401ba28601f8722ce600",
      "bf6d30b9d2454153be8005943170373c",
      "2b226004650e48c0aa52fc9845a61af6",
      "a514ac373258447983d1e650abb216db",
      "f1ccf3a5cfcb431681e2190f5eb23c1e",
      "6bdd17c0d4a541618acac31eab249686",
      "79565d39f6134ca99883ccc576e32b90",
      "62a960f528de48469317ba2d9b982123",
      "54c37dcae69e4dbf98105c7d0b9dd966",
      "f666e8a6f9a445c185109cf7fd548136",
      "fe3bc414e50741cabf466dae55881bb0",
      "c57a1a1103dd48de851f5e14bc61d768",
      "8c3ab31808b745d08a1b296cd9c0de61",
      "5951670fc5f845ddace5968b6b647200",
      "f799732fb08c4aa09d7dc482c2295e79",
      "900202b0391a467cb74399d617c19fec",
      "695e938626924436b9d2fabc0a2e03d9",
      "3e7f8c5b28624acbb1e349ad61df3af9",
      "05f6938445924036bfe4a85133386218",
      "c05512321a614fbfb376694c6566b137",
      "bbdd337ac6c940f7a17e07b12f91fa6f",
      "5425fe70b8944e058d2910a99aea9c32",
      "29066650518946e3a35ddfc6e2c9109a",
      "5040caff406c4bd4850bcc38cec6b887",
      "111effac61a64f819fb76e84ac8efeee",
      "773ca721836347d0b8fcd52525cd6c0d",
      "394505e6d6e8492b9e2bb4d8161251ee",
      "8f6e69a2297c4a03a0244603b8e01fc1",
      "260d98aa9abc4bb8822e60b4af813fb8",
      "0b8032be998b46189fea8699f87bf6e3",
      "c5ac55d1b45d42c3917e43dc668e51e2",
      "efd8dc47299149698a212faea01b039b",
      "5ebfbe5ee59e47d9835f8490738fba99",
      "3b4caa73447d4b7faf0f6d44c76ca745",
      "993d6b4f37be4732ac6dd4b1ea5cd1dd",
      "38c97cf8f27f471499e5c3b1859eb37f",
      "14d16e8fe5114e88a700f2f80bd5c705",
      "c646d8f6a3cf4c83bb5067a80333738e",
      "1f62b2997b0b43bab79e095675dbf871",
      "c2b9b988a8a44df09711609e61128bdd",
      "2e8038200b034fb98102ae2ea1f35906",
      "9d671121819f4b698a9fb894203ab413",
      "6634f44e90ee4893aa90334226c0e5bb",
      "0234be2fd8c748f3a34d5bff86645c96",
      "b56c76cd07bd4cbc837eb16640a8580d",
      "4ee684b272cd47059da9f084419db5b2",
      "ad8587716e3545848246f8a3ba96fdcf",
      "454499afe3804e24b545abd24df3d304",
      "5e2fb55513234aaeb0c937d2f4743d14",
      "6c6d9f4020374b7c8a3410586859e44b",
      "0af1f6f02a7145379e110883a2177809",
      "81c979339ace4505a225853d73bfedf4",
      "f303f4a22fba4130be66c1e60820af45",
      "e7911c0583914cc189d60a9e7a50626d",
      "a4b1955bc86a4da994deb38cc5e9e3b2",
      "31151bb113584dd481150d28745d4d7b",
      "d9e41f80e3254c25962943885b1841b1",
      "61ccc978e42f413d85d0697c3b5e9ed2",
      "e15312dcf8fd4a6dbe35a5d1b4cfcbbf",
      "1f8724becf454db3984e7d6e3f850f28",
      "f47644b543d44f38858ddaf80d36a2d4",
      "dd5cf2f1881e4ba5bbdad9e5c549a562",
      "0fbf06394bec44c3b0f9a90f4c3f0473",
      "cf953cbb002b489ca72f9b7924165929",
      "2608774b71cc4ada9504afcf5bd688fc",
      "24f11feef453489bb729299c8f21552d",
      "193166da5d044c0d927a20d08d0fb2ae",
      "ef2acffae774449c9fb591903e18a2b4",
      "8f08c89755ad4fcca5e1bf0821ce1d95",
      "b6699b6551384453b4ad84c8a0180b3d"
     ]
    },
    "id": "t_lW9e1oyLLT",
    "outputId": "a09f6493-f1ad-4326-8c1c-6f714e94353b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading generator model: google/flan-t5-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb878bb3455435387255bfba0bdadcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ccf3a5cfcb431681e2190f5eb23c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900202b0391a467cb74399d617c19fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394505e6d6e8492b9e2bb4d8161251ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c646d8f6a3cf4c83bb5067a80333738e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2fb55513234aaeb0c937d2f4743d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8724becf454db3984e7d6e3f850f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generator loaded on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "#  LOAD GENERATION MODEL (FLAN-T5)\n",
    "# ========================================================\n",
    "\n",
    "\n",
    "GENERATOR_MODEL = 'google/flan-t5-base'\n",
    "\n",
    "print(f\"Loading generator model: {GENERATOR_MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL)\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(GENERATOR_MODEL)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "generator_model = generator_model.to(device)\n",
    "print(f\"✅ Generator loaded on device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9jc3UyGj7xA",
    "outputId": "66ce56bc-f109-44c7-844f-7913f06a0abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated RAG pipeline defined\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(query, context, max_length=1024):\n",
    "    \"\"\"\n",
    "    Generate answer using FLAN-T5-base with improved prompt and fallback\n",
    "    \"\"\"\n",
    "    # Extract doctor's answer from context\n",
    "    if \"Doctor's Answer:\" in context:\n",
    "        doctor_answer = context.split(\"Doctor's Answer:\")[-1].strip()\n",
    "    elif \"Doctor:\" in context:\n",
    "        doctor_answer = context.split(\"Doctor:\")[-1].strip()\n",
    "    elif \"Doctor's Response:\" in context:\n",
    "        doctor_answer = context.split(\"Doctor's Response:\")[-1].strip()\n",
    "    else:\n",
    "        # If no label found, assume entire context is the answer\n",
    "        doctor_answer = context\n",
    "\n",
    "    # Clean up common artifacts\n",
    "    doctor_answer = doctor_answer.split(\"Thank you for posting your query\")[0].strip()\n",
    "\n",
    "    # # DEBUG: Show what we extracted\n",
    "    # print(f\"📋 Extracted doctor's answer length: {len(doctor_answer)} chars\")\n",
    "    # print(f\"📋 First 200 chars: {doctor_answer[:200]}...\")\n",
    "\n",
    "    # If doctor's answer is too short, something went wrong - use extractive fallback\n",
    "    if len(doctor_answer) < 50:\n",
    "        print(f\"⚠️  Doctor's answer too short ({len(doctor_answer)} chars), using full context\")\n",
    "        doctor_answer = context\n",
    "\n",
    "    # Simplified prompt for FLAN-T5\n",
    "    prompt = f\"\"\"Based on this medical information, answer the patient's question.\n",
    "    Medical Information:\n",
    "{doctor_answer[:1000]}\n",
    "\n",
    "Patient Question: {query[:300]}\n",
    "\n",
    "Medical Answer:\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=1024,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"📊 Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = generator_model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            min_length=50,\n",
    "            num_beams=6,\n",
    "            no_repeat_ngram_size=4,\n",
    "            length_penalty=1.5,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.3\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # print(f\"📝 Generated answer length: {len(answer)} chars\")\n",
    "    # print(f\"📝 First 150 chars: {answer[:150]}...\")\n",
    "\n",
    "    # Quality checks\n",
    "    answer_lower = answer.lower()\n",
    "    query_start = query[:50].lower()\n",
    "\n",
    "    is_bad_generation = (\n",
    "        len(answer) < 30 or\n",
    "        answer_lower.startswith(query_start) or\n",
    "        answer.count(\".\") == 0\n",
    "    )\n",
    "\n",
    "    if is_bad_generation:\n",
    "        print(\"⚠️  Generation failed, returning doctor's answer directly\")\n",
    "        return doctor_answer.strip()\n",
    "\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "print(\"✅ Updated RAG pipeline defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ge_aOJFacegK"
   },
   "outputs": [],
   "source": [
    "def rag_pipeline(query, top_k=10, use_top_n_for_context=1):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with proper context handling\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RAG PIPELINE EXECUTION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Step 1: Hybrid search with reranking\n",
    "    search_results = hybrid_search(\n",
    "        query,\n",
    "        top_k_semantic=20,\n",
    "        top_k_keyword=20,\n",
    "        final_top_k=top_k\n",
    "    )\n",
    "\n",
    "    # Step 2: Check if we have results\n",
    "    if not search_results:\n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': \"I couldn't find relevant information to answer your question.\",\n",
    "            'context': None,\n",
    "            'top_chunks': [],\n",
    "            'method': 'no_results'\n",
    "        }\n",
    "\n",
    "    # Step 3: Get the top chunk\n",
    "    top_chunk = search_results[0]['chunk']\n",
    "\n",
    "    # # DEBUG: Print what we actually have\n",
    "    # print(f\"\\n🔍 DEBUG: Top chunk structure:\")\n",
    "    # print(f\"   Question length: {len(top_chunk['question'])} chars\")\n",
    "    # print(f\"   Answer length: {len(top_chunk['answer'])} chars\")\n",
    "    # print(f\"   Question preview: {top_chunk['question'][:100]}...\")\n",
    "    # print(f\"   Answer preview: {top_chunk['answer'][:100]}...\")\n",
    "\n",
    "    # Build context with FULL answer (this is the fix!)\n",
    "    patient_question = top_chunk['question']\n",
    "    doctor_answer = top_chunk['answer']\n",
    "\n",
    "    # Create properly formatted context\n",
    "    # full_context = f\"Patient Question: {patient_question}\\n\\nDoctor's Answer: {doctor_answer}\"\n",
    "    full_context = f\"Doctor's Answer: {doctor_answer}\"\n",
    "\n",
    "    # print(f\"\\n📊 Context statistics:\")\n",
    "    # print(f\"   Full context length: {len(full_context)} characters\")\n",
    "    # print(f\"   Doctor's answer length: {len(doctor_answer)} characters\")\n",
    "    print(f\"🎯 Top result rerank score: {search_results[0]['rerank_score']:.4f}\")\n",
    "\n",
    "    # # Show context preview\n",
    "    # print(f\"\\n📄 Full context preview (first 500 chars):\")\n",
    "    # print(f\"{full_context[:500]}...\")\n",
    "\n",
    "    # Step 4: Generate answer\n",
    "    print(f\"\\n🤖 Generating answer with FLAN-T5-base...\")\n",
    "    answer = generate_answer(query, full_context, max_length=1024)\n",
    "\n",
    "    # print(f\"\\n✅ Answer generated\")\n",
    "    # print(f\"📝 Final answer length: {len(answer)} characters\")\n",
    "\n",
    "    # Determine method\n",
    "    method = 'extracted' if len(answer) > 500 else 'generated'\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'context': full_context,\n",
    "        'method': method,\n",
    "        'top_chunks': [\n",
    "            {\n",
    "                'chunk_id': r['chunk']['chunk_id'],\n",
    "                'question': r['chunk']['question'][:200] + \"...\",\n",
    "                'answer': r['chunk']['answer'][:200] + \"...\",\n",
    "                'rerank_score': r['rerank_score']\n",
    "            }\n",
    "            for r in search_results[:5]\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ds28tTq1xa6m",
    "outputId": "25806d50-24c7-4a5b-9317-d2319044fd9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'hi my nine year old son had a cough and flu symptons three months ago and the chesty sounding cough ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.3048\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 207\n",
      "\n",
      "💡 GENERATED ANSWER:\n",
      "he has only ever had antibiotics once in his live which suggests he is generally fit and has a cough and flu symptoms three months ago and the chesty sounding cough and green phlegm still remains. It did seem to get better but never totally went and has now picked up again\n",
      "\n",
      "📚 CONTEXT USED:\n",
      "Doctor's Answer: Hi, If the symptoms persist that long this suggests the presence of an allergic element. To treat that kind of allergy you can give him an over the counter antihistamines once daily before going to bed. A cough suppressant as dextromethorphan and an expectorant as Murine will help reduce the cough. Make sure he Chat Doctor.  If the green phlegm persists he might require an antibiotic. Hope I have answered your query. Let me know if I can assist you further....\n",
      "\n",
      "🔍 TOP RETRIEVED CHUNKS:\n",
      "\n",
      "  1. Score: 8.3048\n",
      "     Q: hi my nine year old son had a cough and flu symptons three months ago and the chesty sounding cough and green phlegm still remains. it did seem to get better but never totally went and has now picked ......\n",
      "     A: Hi, If the symptoms persist that long this suggests the presence of an allergic element. To treat that kind of allergy you can give him an over the counter antihistamines once daily before going to be......\n",
      "\n",
      "  2. Score: 2.0759\n",
      "     Q: HI I have a 9month old son he s had a bad cough for 2weeks plus I took him to the doctors 2 weeks ago they said his chest was clear however I went back on friday due to him starting to throw back food......\n",
      "     A: Dear parent, It seems your child is suffering from respiratory infection. He is unable to eat because of mucus he is ingesting. He cannot spit. This mucus is sticky so it doesn't pass ahead in intesti......\n",
      "\n",
      "  3. Score: 1.4316\n",
      "     Q: My son is 9 months old had croup a couple of days ago and yhis morning woke up coughing and sweating. While he would cough he would cry and than i held him and he went back to sleep. He woke up and se......\n",
      "     A: Hello, I understand your concern. I would like to know if he has any other symptoms such as fever or fast breathing? My suggestion is that you count the number of breaths he is taking in a minute (wat......\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# STEP 8: TEST THE COMPLETE SYSTEM\n",
    "# ========================================================\n",
    "\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "   \"hi my nine year old son had a cough and flu symptons three months ago and the chesty sounding cough and green phlegm still remains. it did seem to get better but never totally went and has now picked up again...he has only ever had antibiotics once in his live which suggests he his generally fitand well and active...never short of breath or weezy...so why would this be....\"]\n",
    "for test_query in test_queries:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    result = rag_pipeline(test_query, top_k=10, use_top_n_for_context=1)\n",
    "\n",
    "    # print(f\"\\n📝 QUERY: {result['query']}\")\n",
    "    print(f\"\\n💡 GENERATED ANSWER:\\n{result['answer']}\")\n",
    "    print(f\"\\n📚 CONTEXT USED:\\n{result['context'][:500]}...\")\n",
    "    print(f\"\\n🔍 TOP RETRIEVED CHUNKS:\")\n",
    "    for i, chunk in enumerate(result['top_chunks'][:3], 1):\n",
    "        print(f\"\\n  {i}. Score: {chunk['rerank_score']:.4f}\")\n",
    "        print(f\"     Q: {chunk['question'][:300]}...\")\n",
    "        print(f\"     A: {chunk['answer'][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPB2m4oxas0b",
    "outputId": "9b4ea21c-2b18-49b6-da5f-e5ff788dcbfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conversations for testing...\n",
      "Total loaded: 50\n",
      "\n",
      "Extracted 50 questions from first 50 conversations\n",
      "Testing first 50 questions...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bat...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 5.7516\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "⚠️  Doctor's answer too short (3 chars), using full context\n",
      "📊 Input tokens: 266\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'My baby has been pooing 5-6 times a day for a week. In the last few days it has increased to 7 and t...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.7373\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 220\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hello, My husband is taking Oxycodone due to a broken leg/surgery. He has been taking this pain medi...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.2584\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 295\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'lump under left nipple and stomach pain (male) Hi,I have recently noticed a few weeks ago a lump und...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 9.0071\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 236\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I have a 5 month old baby who is very congested with a terrible cough. Its rattly/raspy and croupy s...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 9.2747\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 234\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I am F 38 in good shape work out (do triathlons) regular but have had back pain from different reaso...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.4298\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 199\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'sir, MY uncle has ILD-Interstitial Lung disease.from my research over google i found that the cause ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.7801\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 263\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'my husband was working on a project in the house and all of a sudden a bump about the size of a half...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.2364\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 195\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'hi my nine year old son had a cough and flu symptons three months ago and the chesty sounding cough ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.3048\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 207\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'gyno problemsfor the past few months, I have been having issues with my vagina. there always seems t...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 6.8039\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 212\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I have found that I have an allergy to leotensin. They have taken me off of everything....I found in...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.2784\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 200\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hi my name is Wendy I ve been having an issue for the last couple of months or so I been having seve...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.3443\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 192\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'hye, My aunt is having shortness of breath and she is on vent now. Her breast showed some kind of in...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 9.0061\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 241\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'i am 28 yrs old, i got married before 6 year, i was infected by pulmonary tuberculosis before 6 year...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.4882\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 252\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I sprained my foot on Friday. I stepped on what I thought was lawn. Instead I was a bit on the lawn,...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.3609\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 276\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'hi, i have been recently diagnosed with H pylori.. i have been give the triple treatment of calithra...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.8170\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 312\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I had a alt reading about 2+ months ago of 43 then retested today and got a 60 alt reading my other ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: -1.3370\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 240\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'i had surgery done 5 months ago for scar tissue and adhesion removal. my gynecologist just found a c...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.1132\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 331\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hi, I had a subarachnoid bleed and coiling of brain aneurysm last year. I am having some major bilat...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.2094\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 220\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Almost everytime I eat, it burns when I swallow. Not in my throat but in my chest. It burns all the ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.4189\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 321\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'for the pst  6 days  i have been haing  upper  abdome  distress ...  i have had gall bladder  surger...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.4055\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 314\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I have a hole its about the size of a dime at the beggining of my butt crack (gross I know) but Im j...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.8528\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 250\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Dear Doctor, We are trying to have a baby and gave it a big try this time but as we are planning to ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.5171\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 221\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hello, I am in the middle of a serious anxiety/panic attack as I just became aware that I may have a...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 0.4622\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 294\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hi hope you can helpive been getting chest pains, not severe but a constant pain, can happen when im...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.9306\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 248\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Im 28 year old and suffering from male baldness pattern. I started using RICHFEEL tricology centres ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.9371\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 219\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I too fell and hurt my left side of my chest. I have a large lump and I am very black and blue. I ne...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 9.0186\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 197\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I have had a few small hard painless bumps where my cheek goes to my gums. this may sound terrible b...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.2715\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 241\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'my mother is 80 yrs. detected with vulva cancer. vulva is swollen red ,around discolourisation, whit...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 9.0270\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 210\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Suddenly this morning I got really excruciating lower right back pain. I was standing in the kitchen...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.5983\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 326\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hey I am on my third antibiotic and still coughing and my ribs are killing me especially right side....'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.4569\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 268\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hi My mrs has this heart murmur, which is causing the blackouts, and she has panic attacks. Yesterda...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.5961\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 240\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I am on metformin 1000mg bid for pcos & was prescribed Percocet 10/650 for a severely displaced frac...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.0633\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 234\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I stopped taking microgestin, mid-pack, on 12/5/13 after being on it for 2 1/2 months. I had withdra...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 5.7313\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 351\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I am going through Jaundice due to Hepatitis E, my stomach has gone too much weak, at time a lot of ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 6.2792\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 281\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'My family has recently gotten the scabies mite... WE have cleaned our house top to bott om with hot ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 5.7605\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 225\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hi, didn t expect to get a real person, but since your are here, I will tell you about symptoms I ha...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 4.8815\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 288\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I fell on uneven sidewalk on July 3rd and hurt my knee: I had no problem walking and I didn t have m...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 9.2904\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 224\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Dear doctor, i am 33 male recently i have undergone pre employment medical check up and it was found...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 3.2829\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 329\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hello, My name is Wanda I am a 40year old female with type one diabetes (have had it for 7 years ) I...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.7697\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 243\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I had a partial root canal roughly 36 hours ago on my lower left rear molar (wisdom teeth extracted ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 4.9464\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 346\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'My doctor had issue with finding the baby during my transvaginal ultrasound is that because I had to...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 9.6024\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 206\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I hit a volleyball this after with the underside if my hand (made a fist) it bent my wrist backward ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.0119\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 247\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I have a pacemaker, due to atrial fibrillation, and have been put on coumadin. It does not seem to t...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 6.3786\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 198\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'My younger brother liver & spleen got enlarged. Hepatitis B&C found in blood. Mild ascitis & SOL on ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.8499\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 314\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hi Over the past 2 days I have had a sore jaw on the right side only. Doesn t feel sore on the outsi...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 7.4869\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 345\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hi, Can anyone give me the solution for Aplastic Anemia ? my 4 years old daughter suffering from Apl...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 9.4763\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 238\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'I HAVE HAD SEVERAL TIAS BEFORE THE AGE OF 40 AND JUST RECENTLY I HAD DIFFICULTIES ORGANSIZING MY THO...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.0626\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 344\n",
      "⚠️  Generation failed, returning doctor's answer directly\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'My son is 1 yr 8 months old...he is not walking on his own...so dr suggested MRI n MRI report shows ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.2148\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 234\n",
      "\n",
      "================================================================================\n",
      "RAG PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "Query: 'Hi. My 18month old has had red and swollen eyes for about 5 days. I took him to his doctor and they ...'\n",
      "------------------------------------------------------------\n",
      "🎯 Top result rerank score: 8.8211\n",
      "\n",
      "🤖 Generating answer with FLAN-T5-base...\n",
      "📊 Input tokens: 212\n",
      "\n",
      "================================================================================\n",
      "Testing complete! Processed 50 queries.\n",
      "Results saved to: rag_test_results_20251107_191921.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# STEP 8: TEST THE COMPLETE SYSTEM WITH DATASET QUERIES(only for 50 data)\n",
    "# =======================================================================\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the first 50 conversations and extract questions\n",
    "print(\"Loading conversations for testing...\")\n",
    "test_conversations = load_conversations(input_file, limit=50)\n",
    "\n",
    "# Extract all questions from the first 50 conversations\n",
    "test_queries = []\n",
    "for conv_id, conversation in enumerate(test_conversations):\n",
    "    chunks = chunk_conversation(conversation, conv_id)\n",
    "    for chunk in chunks:\n",
    "        if chunk['question']:  # Only add non-empty questions\n",
    "            test_queries.append(chunk['question'])\n",
    "\n",
    "print(f\"\\nExtracted {len(test_queries)} questions from first 50 conversations\")\n",
    "print(f\"Testing first 50 questions...\\n\")\n",
    "\n",
    "# Prepare CSV file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = f\"rag_test_results_{timestamp}.csv\"\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['query_number', 'question', 'generated_answer', 'context_snippet', 'top_score']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Test the first 50 questions\n",
    "    for idx, test_query in enumerate(test_queries[:50], 1):\n",
    "        # print(\"\\n\" + \"=\"*80)\n",
    "        # print(f\"QUERY {idx}/{min(50, len(test_queries))}\")\n",
    "        # print(\"=\"*80)\n",
    "\n",
    "        result = rag_pipeline(test_query, top_k=10, use_top_n_for_context=1)\n",
    "\n",
    "        # Write to CSV\n",
    "        writer.writerow({\n",
    "            'query_number': idx,\n",
    "            'question': test_query,\n",
    "            'generated_answer': result['answer'],\n",
    "            'context_snippet': result['context'][:500],  # First 500 chars of context\n",
    "            'top_score': result['top_chunks'][0]['rerank_score'] if result['top_chunks'] else 0\n",
    "        })\n",
    "\n",
    "        # Flush to ensure data is written immediately\n",
    "        csvfile.flush()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Testing complete! Processed {min(50, len(test_queries))} queries.\")\n",
    "print(f\"Results saved to: {csv_filename}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbOYUng4asxB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
